"""
æ”¹è¿›çš„æ‰¹é‡å¤„ç†å™¨
æä¾›æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
"""

import logging
import time
from typing import Dict, Any, List, Tuple
from django.db import transaction
from django.utils import timezone
from django_async_manager import get_background_task
from .atomic_state_manager import atomic_state_manager

logger = logging.getLogger(__name__)
background_task = get_background_task()


class BatchProcessingError(Exception):
    """æ‰¹é‡å¤„ç†é”™è¯¯åŸºç±»"""
    pass


class BatchValidationError(BatchProcessingError):
    """æ‰¹é‡å¤„ç†éªŒè¯é”™è¯¯"""
    pass


class BatchExecutionError(BatchProcessingError):
    """æ‰¹é‡å¤„ç†æ‰§è¡Œé”™è¯¯"""
    pass


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self):
        self.max_batch_size = 20
        self.default_concurrent = 2
        self.max_concurrent_per_user = 5
        self.task_timeout = 300  # 5åˆ†é’Ÿ

    def validate_batch_request(self, media_ids: List[int], model_name: str = None,
                              analysis_options: Dict[str, Any] = None) -> Dict[str, Any]:
        """éªŒè¯æ‰¹é‡è¯·æ±‚å‚æ•°"""
        errors = []
        warnings = []

        # éªŒè¯åª’ä½“IDåˆ—è¡¨
        if not media_ids or not isinstance(media_ids, list):
            errors.append("media_ids å¿…é¡»æ˜¯éç©ºæ•°ç»„")
            return {'valid': False, 'errors': errors, 'warnings': warnings}

        if len(media_ids) > self.max_batch_size:
            errors.append(f"æ‰¹é‡å¤§å°è¶…è¿‡é™åˆ¶ï¼Œæœ€å¤šæ”¯æŒ {self.max_batch_size} ä¸ªæ–‡ä»¶")

        if len(media_ids) == 0:
            errors.append("åª’ä½“IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        # æ£€æŸ¥é‡å¤ID
        if len(media_ids) != len(set(media_ids)):
            warnings.append("åª’ä½“IDåˆ—è¡¨ä¸­åŒ…å«é‡å¤é¡¹")

        # éªŒè¯å¹¶å‘æ§åˆ¶å‚æ•°
        analysis_options = analysis_options or {}
        concurrency_errors = self._validate_concurrency_options(analysis_options)
        errors.extend(concurrency_errors)

        # éªŒè¯æ¨¡å‹åç§°
        if model_name and not isinstance(model_name, str):
            errors.append("æ¨¡å‹åç§°å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings,
            'media_count': len(media_ids)
        }

    def _validate_concurrency_options(self, options: Dict[str, Any]) -> List[str]:
        """éªŒè¯å¹¶å‘æ§åˆ¶é€‰é¡¹"""
        errors = []

        if 'max_concurrent' in options:
            max_concurrent = options['max_concurrent']
            if not isinstance(max_concurrent, int) or not 1 <= max_concurrent <= self.max_concurrent_per_user:
                errors.append(f'max_concurrentå¿…é¡»åœ¨1-{self.max_concurrent_per_user}ä¹‹é—´')

        if 'use_concurrency' in options:
            use_concurrency = options['use_concurrency']
            if not isinstance(use_concurrency, bool):
                errors.append('use_concurrencyå¿…é¡»æ˜¯å¸ƒå°”å€¼')

        return errors

    @transaction.atomic
    def prepare_batch_tasks(self, user, media_ids: List[int], model_name: str = None,
                          analysis_options: Dict[str, Any] = None) -> Tuple[List, List, Dict[str, Any]]:
        """å‡†å¤‡æ‰¹é‡ä»»åŠ¡ï¼ˆåŸå­æ€§æ“ä½œï¼‰"""
        from media.models import Media
        from ..models import OllamaAIModel, OllamaImageAnalysis

        valid_tasks = []
        validation_errors = []

        try:
            # éªŒè¯å¹¶è·å–åª’ä½“æ–‡ä»¶
            valid_media_items = []
            for media_id in media_ids:
                try:
                    media = Media.objects.get(id=media_id, user=user)
                    valid_media_items.append(media)
                except Media.DoesNotExist:
                    validation_errors.append({
                        'media_id': media_id,
                        'error': 'åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æƒè®¿é—®'
                    })

            # å³ä½¿æ²¡æœ‰æœ‰æ•ˆåª’ä½“æ–‡ä»¶ï¼Œä¹Ÿç»§ç»­å¤„ç†ï¼Œè®©è§†å›¾å±‚èƒ½å¤Ÿè¿”å›åŒ…å«è·³è¿‡é¡¹çš„å“åº”
            # è¿™æ ·å¯ä»¥ä¿æŒAPIå“åº”æ ¼å¼çš„ä¸€è‡´æ€§

            # è·å–æˆ–éªŒè¯æ¨¡å‹
            model = self._get_or_validate_model(user, model_name)
            if not model:
                raise BatchValidationError("æ²¡æœ‰å¯ç”¨çš„åˆ†ææ¨¡å‹")

            # ä¸ºæ¯ä¸ªåª’ä½“æ–‡ä»¶åˆ›å»ºåˆ†æä»»åŠ¡ - å…è®¸é‡å¤ä»»åŠ¡
            for media in valid_media_items:
                try:
                    # ç›´æ¥åˆ›å»ºæ–°çš„åˆ†æä»»åŠ¡ï¼Œä¸æ£€æŸ¥é‡å¤
                    # æ¯æ¬¡æ‰¹é‡åˆ†æéƒ½åº”è¯¥åˆ›å»ºæ–°çš„å¼‚æ­¥ä»»åŠ¡
                    analysis = OllamaImageAnalysis.objects.create(
                        media=media,
                        model=model,
                        analysis_options=analysis_options or {},
                        prompt=None,
                        status='pending'
                    )

                    valid_tasks.append(analysis)
                    logger.info(f"âœ… åˆ›å»ºåˆ†æä»»åŠ¡: media_id={media.id}, analysis_id={analysis.id}")

                except Exception as e:
                    validation_errors.append({
                        'media_id': media.id,
                        'error': f"åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}"
                    })

            # æ³¨æ„ï¼šç§»é™¤äº†skipped_itemså¤„ç†ï¼Œå› ä¸ºç°åœ¨å…è®¸é‡å¤ä»»åŠ¡
            # æ¯ä¸ªåª’ä½“æ–‡ä»¶éƒ½ä¼šåˆ›å»ºæ–°çš„åˆ†æä»»åŠ¡

            summary = {
                'total_requested': len(media_ids),
                'valid_tasks': len(valid_tasks),
                'validation_errors': len(validation_errors),
                'skipped_items': 0  # ç§»é™¤è·³è¿‡é€»è¾‘ï¼Œæ€»æ˜¯0
            }

            logger.info(f"æ‰¹é‡ä»»åŠ¡å‡†å¤‡å®Œæˆ: {summary}")
            return valid_tasks, validation_errors, summary

        except Exception as e:
            logger.error(f"æ‰¹é‡ä»»åŠ¡å‡†å¤‡å¤±è´¥: {str(e)}")
            raise BatchValidationError(f"æ‰¹é‡ä»»åŠ¡å‡†å¤‡å¤±è´¥: {str(e)}")

    def _get_or_validate_model(self, user, model_name: str = None):
        """è·å–æˆ–éªŒè¯æ¨¡å‹"""
        from ..models import OllamaAIModel

        queryset = OllamaAIModel.objects.filter(
            endpoint__created_by=user,
            is_active=True,
            is_vision_capable=True
        )

        if model_name:
            queryset = queryset.filter(name=model_name)

        # ä¼˜å…ˆä½¿ç”¨é»˜è®¤æ¨¡å‹
        model = queryset.filter(is_default=True).first()
        if not model:
            model = queryset.first()

        return model

    def execute_batch_processing(self, user, valid_tasks: List, analysis_options: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ‰¹é‡å¤„ç†"""
        try:
            if not valid_tasks:
                raise BatchExecutionError("æ²¡æœ‰æœ‰æ•ˆçš„ä»»åŠ¡éœ€è¦å¤„ç†")

            # å‡†å¤‡æ‰§è¡Œå‚æ•°
            media_ids = [task.media.id for task in valid_tasks]
            model_name = valid_tasks[0].model.name  # æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨ç›¸åŒæ¨¡å‹

            # åˆ›å»º media_id åˆ° analysis_id çš„æ˜ å°„
            media_to_analysis = {task.media.id: task.id for task in valid_tasks}

            # å¯¼å…¥å¹¶å‘æ§åˆ¶å™¨
            from .concurrency_controller import concurrency_controller

            # å‡†å¤‡æ‰§è¡Œå™¨å›è°ƒ
            from .image_analyzer import OllamaImageAnalyzer
            analyzer = OllamaImageAnalyzer()

            def executor_callback(analysis_obj, prompt_text):
                return analyzer._prepare_single_analysis(analysis_obj, prompt_text)

            # æ‰§è¡Œæ‰¹é‡å¤„ç†
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡å¤„ç†: {len(media_ids)} ä¸ªæ–‡ä»¶ï¼Œç”¨æˆ·: {user.id}")
            start_time = time.time()

            batch_result = concurrency_controller.process_batch_images(
                user_id=user.id,
                media_ids=media_ids,
                model_name=model_name,
                analysis_options=analysis_options,
                executor_callback=executor_callback
            )

            processing_time = int((time.time() - start_time) * 1000)

            logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {batch_result['success_count']} ä¸ªï¼Œ"
                       f"å¤±è´¥ {batch_result['error_count']} ä¸ªï¼Œè€—æ—¶: {processing_time}ms")

            # å¢å¼ºç»“æœæ ¼å¼ï¼Œæ·»åŠ  analysis_ids
            enhanced_results = []
            for result in batch_result['results']:
                media_id = result.get('media_id')
                enhanced_result = result.copy()
                enhanced_result['analysis_id'] = media_to_analysis.get(media_id)
                enhanced_results.append(enhanced_result)

            # å¢å¼ºå¤±è´¥é¡¹ï¼Œæ·»åŠ  analysis_ids
            enhanced_failed_items = []
            for failed_item in batch_result['failed_items']:
                media_id = failed_item.get('media_id')
                enhanced_failed_item = failed_item.copy()
                enhanced_failed_item['analysis_id'] = media_to_analysis.get(media_id)
                enhanced_failed_items.append(enhanced_failed_item)

            return {
                'success_count': batch_result['success_count'],
                'error_count': batch_result['error_count'],
                'results': enhanced_results,
                'failed_items': enhanced_failed_items,
                'processing_time_ms': processing_time,
                'media_ids': media_ids,
                'analysis_ids': list(media_to_analysis.values()),
                'media_analysis_mapping': media_to_analysis
            }

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise BatchExecutionError(f"æ‰¹é‡å¤„ç†æ‰§è¡Œå¤±è´¥: {str(e)}")

    def handle_batch_failure(self, valid_tasks: List, error: Exception):
        """å¤„ç†æ‰¹é‡å¤±è´¥æƒ…å†µ"""
        try:
            # å–æ¶ˆæ‰€æœ‰å·²æäº¤çš„ä»»åŠ¡
            analysis_ids = [task.id for task in valid_tasks]
            cancelled_result = atomic_state_manager.batch_update_status(
                analysis_ids=analysis_ids,
                from_status=['pending', 'processing'],
                to_status='failed',
                error_message=f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(error)}"
            )

            logger.info(f"ğŸš« æ‰¹é‡å¤±è´¥å¤„ç†: å–æ¶ˆäº† {cancelled_result['success_count']} ä¸ªä»»åŠ¡")

            return cancelled_result

        except Exception as cleanup_error:
            logger.error(f"âŒ æ‰¹é‡å¤±è´¥æ¸…ç†æ“ä½œå¤±è´¥: {str(cleanup_error)}")
            return {'success_count': 0, 'error_count': len(valid_tasks)}

    def _start_async_batch_processing(self, user, valid_tasks: List, analysis_options: Dict[str, Any]) -> None:
        """å¯åŠ¨åŸºäºå¹¶å‘æ§åˆ¶çš„å¼‚æ­¥æ‰¹é‡å¤„ç† - æ‰€æœ‰ä»»åŠ¡éƒ½åœ¨workerä¸­å¹¶å‘è¿è¡Œ"""
        try:
            from .concurrency_controller import concurrency_controller

            logger.info(f"ğŸš€ å¯åŠ¨åŸºäºå¹¶å‘æ§åˆ¶çš„æ‰¹é‡å¤„ç†: {len(valid_tasks)} ä¸ªä»»åŠ¡")

            # è·å–å¹¶å‘æ§åˆ¶å‚æ•°
            max_concurrent = analysis_options.get('max_concurrent', self.default_concurrent)

            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            task_data = []
            for analysis in valid_tasks:
                task_data.append({
                    'analysis_id': analysis.id,
                    'media_id': analysis.media.id,
                    'analysis': analysis
                })

            # ä½¿ç”¨å¹¶å‘æ§åˆ¶å™¨æ‰§è¡Œæ‰¹é‡å¤„ç†
            def task_executor(analysis_obj):
                """å•ä¸ªä»»åŠ¡çš„æ‰§è¡Œå‡½æ•° - ç›´æ¥æ‰§è¡Œå›¾ç‰‡åˆ†æï¼Œä¸åˆ›å»ºå¼‚æ­¥ä»»åŠ¡"""
                try:
                    from .image_analyzer import OllamaImageAnalyzer

                    logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å›¾ç‰‡åˆ†æ: analysis_id={analysis_obj.id}")

                    # ç«‹å³æ ‡è®°ä»»åŠ¡å¼€å§‹å¤„ç†ï¼ˆé¿å…çŠ¶æ€å¡ä½ï¼‰
                    try:
                        analysis_obj.mark_as_started()
                        analysis_obj.save(update_fields=['status', 'started_at'])
                    except Exception as save_error:
                        logger.error(f"âŒ æ— æ³•æ ‡è®°ä»»åŠ¡å¼€å§‹: analysis_id={analysis_obj.id}, error={str(save_error)}")

                    # ç›´æ¥æ‰§è¡Œå›¾ç‰‡åˆ†æï¼Œä½†å¼ºåˆ¶ä½¿ç”¨ä¸²è¡Œæ¨¡å¼é¿å…åŒé‡å¹¶å‘æ§åˆ¶
                    analyzer = OllamaImageAnalyzer()
                    # ä¸´æ—¶ä¿®æ”¹åˆ†æé€‰é¡¹ï¼Œå¼ºåˆ¶ä¸²è¡Œæ‰§è¡Œ
                    original_options = analysis_obj.analysis_options.copy()
                    analysis_obj.analysis_options['use_concurrency'] = False
                    # ä¿å­˜é€‰é¡¹æ›´æ”¹
                    analysis_obj.save(update_fields=['analysis_options'])

                    result = analyzer.analyze(analysis_obj)
                    # æ¢å¤åŸå§‹é€‰é¡¹
                    analysis_obj.analysis_options = original_options
                    analysis_obj.save(update_fields=['analysis_options'])

                    # å¤„ç†åˆ†æç»“æœ
                    if result.get('success'):
                        # æ ‡è®°ä¸ºå·²å®Œæˆ
                        processing_time = result.get('processing_time_ms')
                        analysis_obj.mark_as_completed(processing_time)

                        # æ›´æ–°åª’ä½“æ–‡ä»¶ä¿¡æ¯
                        if result.get('result'):
                            analysis_obj.update_media_with_analysis_result(result['result'])

                        logger.info(f"âœ… å¹¶å‘å›¾ç‰‡åˆ†æå®Œæˆ: analysis_id={analysis_obj.id}")
                    else:
                        # æ ‡è®°ä¸ºå¤±è´¥
                        error_message = result.get('error', 'æœªçŸ¥é”™è¯¯')
                        analysis_obj.mark_as_failed(error_message)
                        logger.error(f"âŒ å›¾ç‰‡åˆ†æå¤±è´¥: analysis_id={analysis_obj.id}, é”™è¯¯: {error_message}")

                    # ä¿å­˜çŠ¶æ€æ›´æ”¹
                    analysis_obj.save()

                    return {
                        'success': result.get('success', False),
                        'analysis_id': analysis_obj.id,
                        'result': result,
                        'media_id': analysis_obj.media.id
                    }

                except Exception as e:
                    logger.error(f"âŒ å¹¶å‘å›¾ç‰‡åˆ†æå¤±è´¥: analysis_id={analysis_obj.id}, error={str(e)}")
                    try:
                        analysis_obj.mark_as_failed(f'å¹¶å‘å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}')
                        analysis_obj.save()
                    except Exception as save_error:
                        logger.error(f"âŒ æ— æ³•æ ‡è®°ä»»åŠ¡å¤±è´¥: analysis_id={analysis_obj.id}, error={str(save_error)}")

                    return {
                        'success': False,
                        'analysis_id': analysis_obj.id,
                        'error': str(e),
                        'media_id': analysis_obj.media.id
                    }

            # é€šè¿‡å¹¶å‘æ§åˆ¶å™¨æäº¤æ‰€æœ‰ä»»åŠ¡
            # æ·»åŠ æ•°æ®åº“è¿æ¥æ£€æŸ¥å’Œé”™è¯¯å¤„ç†
            try:
                futures = []
                submitted_count = 0

                for task_info in task_data:
                    try:
                        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
                        from django.db import connection
                        if connection.connection and connection.connection.closed:
                            connection.connection = None

                        future = concurrency_controller.submit_task(
                            user_id=user.id,
                            task_func=task_executor,
                            analysis_obj=task_info['analysis']
                        )
                        futures.append(future)
                        submitted_count += 1

                        logger.debug(f"âœ… æˆåŠŸæäº¤ä»»åŠ¡: analysis_id={task_info['analysis'].id}")

                    except Exception as submit_error:
                        logger.error(f"âŒ æäº¤ä»»åŠ¡å¤±è´¥: analysis_id={task_info['analysis'].id}, error={str(submit_error)}")
                        # æ ‡è®°å¤±è´¥çš„ä»»åŠ¡
                        try:
                            task_info['analysis'].mark_as_failed(f'ä»»åŠ¡æäº¤å¤±è´¥: {str(submit_error)}')
                            task_info['analysis'].save()
                        except:
                            pass

                logger.info(f"ğŸ¯ å¹¶å‘æ‰¹é‡å¤„ç†å¯åŠ¨: æˆåŠŸæäº¤ {submitted_count}/{len(task_data)} ä¸ªä»»åŠ¡åˆ°å¹¶å‘æ§åˆ¶å™¨")

                # å¦‚æœæ²¡æœ‰ä»»ä½•ä»»åŠ¡è¢«æˆåŠŸæäº¤ï¼Œå¤„ç†å‰©ä½™ä»»åŠ¡
                if submitted_count == 0:
                    logger.error("âŒ æ²¡æœ‰ä»»åŠ¡èƒ½å¤Ÿè¢«æˆåŠŸæäº¤åˆ°å¹¶å‘æ§åˆ¶å™¨")
                    self.handle_batch_failure(valid_tasks, "å¹¶å‘æ§åˆ¶å™¨æ— æ³•æ¥å—ä»»ä½•ä»»åŠ¡")

            except Exception as e:
                logger.error(f"âŒ å¹¶å‘æ‰¹é‡å¤„ç†å¯åŠ¨å¤±è´¥: {str(e)}")
                # å¦‚æœå¹¶å‘æ§åˆ¶å¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹å¼

        except Exception as e:
            logger.error(f"âŒ å¹¶å‘æ‰¹é‡å¤„ç†å¯åŠ¨å¤±è´¥: {str(e)}")
            # å¦‚æœå¹¶å‘æ§åˆ¶å¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹å¼
            try:
                logger.info("ğŸ”„ å›é€€åˆ°ç›´æ¥å¼‚æ­¥ä»»åŠ¡æ¨¡å¼")
                self._fallback_to_direct_async_tasks(user, valid_tasks)
            except Exception as fallback_error:
                logger.error(f"âŒ å›é€€å¤„ç†ä¹Ÿå¤±è´¥: {str(fallback_error)}")
                self.handle_batch_failure(valid_tasks, fallback_error)

    def _fallback_to_direct_async_tasks(self, user, valid_tasks: List) -> None:
        """å›é€€åˆ°ç›´æ¥å¼‚æ­¥ä»»åŠ¡æ¨¡å¼ï¼ˆä¸ç»è¿‡å¹¶å‘æ§åˆ¶å™¨ï¼‰"""
        from .async_tasks import analyze_image_with_ollama_task

        for analysis in valid_tasks:
            try:
                task = analyze_image_with_ollama_task.run_async(analysis_id=analysis.id)
                analysis.task_id = task.id
                analysis.save(update_fields=['task_id'])

                logger.info(f"ğŸ”„ å›é€€ä»»åŠ¡åˆ›å»º: analysis_id={analysis.id}, task_id={task.id}")

            except Exception as e:
                logger.error(f"âŒ å›é€€ä»»åŠ¡åˆ›å»ºå¤±è´¥: analysis_id={analysis.id}, error={str(e)}")
                analysis.mark_as_failed(f'ä»»åŠ¡åˆ›å»ºå¤±è´¥: {str(e)}')

    def analyze_images_with_concurrency_task(self, user_id: int, media_ids: List[int],
                                             model_name: str, analysis_options: Dict[str, Any] = None,
                                             prompt: str = None) -> Dict[str, Any]:
        """
        å›¾ç‰‡å¹¶å‘æ‰¹é‡åˆ†æä»»åŠ¡
        æä¾›æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
        """
        logger.info(f"ğŸš€ å¼€å§‹å›¾ç‰‡å¹¶å‘æ‰¹é‡åˆ†æ: {len(media_ids)} å¼ å›¾ç‰‡ï¼Œç”¨æˆ·: {user_id}")

        try:
            from django.contrib.auth import get_user_model
            User = get_user_model()

            # è·å–ç”¨æˆ·
            user = User.objects.get(id=user_id)

            # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨å®ä¾‹
            processor = BatchProcessor()

            # éªŒè¯æ‰¹é‡è¯·æ±‚
            validation_result = processor.validate_batch_request(media_ids, model_name, analysis_options)
            if not validation_result['valid']:
                return {
                    'success': False,
                    'error': f"æ‰¹é‡è¯·æ±‚éªŒè¯å¤±è´¥: {'; '.join(validation_result['errors'])}",
                    'validation_errors': validation_result['errors']
                }

            # å‡†å¤‡æ‰¹é‡ä»»åŠ¡ï¼ˆåŸå­æ€§æ“ä½œï¼‰
            valid_tasks, validation_errors, summary = processor.prepare_batch_tasks(
                user=user,
                media_ids=media_ids,
                model_name=model_name,
                analysis_options=analysis_options
            )

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä»»åŠ¡ï¼Œè¿”å›ç»“æœ
            if not valid_tasks:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰å¯å¤„ç†çš„ä»»åŠ¡',
                    'validation_errors': validation_errors,
                    'summary': summary
                }

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç«‹å³å¯åŠ¨å¼‚æ­¥å¤„ç†ï¼Œä¸ç­‰å¾…ç»“æœ
            try:
                # åˆ›å»º media_id åˆ° analysis_id çš„æ˜ å°„
                media_to_analysis = {task.media.id: task.id for task in valid_tasks}

                # ç«‹å³å¯åŠ¨å¹¶å‘æ‰¹é‡å¤„ç†ï¼ˆåœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼‰
                processor._start_async_batch_processing(user, valid_tasks, analysis_options)

                # ç«‹å³è¿”å›ä»»åŠ¡å¯åŠ¨ä¿¡æ¯ï¼Œä¸ç­‰å¾…å¤„ç†å®Œæˆ
                response = {
                    'success': True,
                    'batch_started': True,
                    'summary': summary,
                    'analysis_ids': list(media_to_analysis.values()),
                    'media_analysis_mapping': media_to_analysis,
                    'validation_errors': validation_errors if validation_errors else None,
                    'warnings': validation_result.get('warnings', []),
                    'message': 'æ‰¹é‡åˆ†æä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°å¼‚æ­¥å¤„ç†'
                }

                logger.info(f"ğŸš€ æ‰¹é‡åˆ†æä»»åŠ¡å·²å¯åŠ¨: {summary['total_requested']} ä¸ªæ–‡ä»¶ï¼Œ{len(valid_tasks)} ä¸ªæœ‰æ•ˆä»»åŠ¡")
                return response

            except Exception as e:
                # å¤„ç†ä»»åŠ¡å¯åŠ¨é”™è¯¯
                logger.error(f"âŒ æ‰¹é‡ä»»åŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")
                processor.handle_batch_failure(valid_tasks, e)

                return {
                    'success': False,
                    'error': f"æ‰¹é‡ä»»åŠ¡å¯åŠ¨å¤±è´¥: {str(e)}",
                    'summary': summary,
                    'validation_errors': validation_errors,
                    'cancelled_count': len(valid_tasks)
                }

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ†æä»»åŠ¡å¼‚å¸¸: {str(e)}")

            # å°è¯•æ¸…ç†èµ„æº
            try:
                if 'valid_tasks' in locals():
                    processor.handle_batch_failure(valid_tasks, e)
            except:
                pass

            return {
                'success': False,
                'error': f"æ‰¹é‡åˆ†æä»»åŠ¡å¼‚å¸¸: {str(e)}",
                'media_ids': media_ids,
                'user_id': user_id
            }

    def get_batch_status_summary(self, user) -> Dict[str, Any]:
        """è·å–æ‰¹é‡çŠ¶æ€æ‘˜è¦"""
        try:
            # ä½¿ç”¨åŸå­çŠ¶æ€ç®¡ç†å™¨è·å–ç»Ÿè®¡ä¿¡æ¯
            user_stats = atomic_state_manager.get_user_task_statistics(user.id)

            # æ·»åŠ æ‰¹é‡ç‰¹å®šä¿¡æ¯
            from django_async_manager.models import Task
            batch_tasks = Task.objects.filter(
                name__startswith='improved_analyze_images_with_concurrency_task',
                status__in=['PENDING', 'RUNNING', 'RETRY']
            ).count()

            return {
                'user_task_stats': user_stats,
                'active_batch_tasks': batch_tasks,
                'system_status': 'healthy' if batch_tasks < 5 else 'busy'
            }

        except Exception as e:
            logger.error(f"âŒ è·å–æ‰¹é‡çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                'user_task_stats': {},
                'active_batch_tasks': 0,
                'system_status': 'error',
                'error': str(e)
            }


# å…¨å±€æ‰¹é‡å¤„ç†å™¨å®ä¾‹
batch_processor = BatchProcessor()


# Djangoå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨å…¼å®¹æ€§åŒ…è£…å™¨ï¼ˆå¿…éœ€ï¼‰
@background_task(max_retries=2, retry_delay=60)
def analyze_images_with_concurrency_task(user_id: int, media_ids: List[int],
                                       model_name: str, analysis_options: Dict[str, Any] = None,
                                       prompt: str = None) -> Dict[str, Any]:
    """
    Djangoå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨å…¼å®¹æ€§åŒ…è£…å™¨

    ç”±äºDjangoå¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿåªèƒ½è°ƒç”¨æ¨¡å—çº§å‡½æ•°ï¼Œæ— æ³•è°ƒç”¨ç±»æ–¹æ³•ï¼Œ
    å› æ­¤éœ€è¦è¿™ä¸ªåŒ…è£…å™¨å‡½æ•°æ¥æ¡¥æ¥åˆ°æ‰¹é‡å¤„ç†å™¨å®ä¾‹ã€‚

    Args:
        user_id: ç”¨æˆ·ID
        media_ids: åª’ä½“IDåˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        analysis_options: åˆ†æé€‰é¡¹
        prompt: è‡ªå®šä¹‰æç¤ºè¯

    Returns:
        Dict[str, Any]: ä»»åŠ¡æ‰§è¡Œç»“æœ
    """
    # åˆ›å»ºæ–°çš„æ‰¹é‡å¤„ç†å™¨å®ä¾‹ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
    processor = BatchProcessor()
    return processor.analyze_images_with_concurrency_task(
        user_id, media_ids, model_name, analysis_options, prompt
    )