"""
åŸå­çŠ¶æ€ç®¡ç†å™¨
æä¾›æ•°æ®åº“äº‹åŠ¡ä¿æŠ¤å’ŒåŸå­æ€§çŠ¶æ€æ“ä½œ
"""

import logging
import time
import random
from typing import Dict, Any, Optional, List
from django.db import transaction, models, DatabaseError
from django.db.models import F, Q
from django.core.cache import cache
from django.utils import timezone

logger = logging.getLogger(__name__)


class StateManager:
    """åŸå­çŠ¶æ€ç®¡ç†å™¨"""

    def __init__(self):
        self.cache_timeout = 30  # 30ç§’ç¼“å­˜è¶…æ—¶ï¼Œæé«˜å®æ—¶æ€§
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.base_delay = 0.1  # åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

    def _retry_with_backoff(self, func, *args, **kwargs):
        """
        å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶
        ä¸»è¦ç”¨äºå¤„ç†æ•°æ®åº“é”å®šé—®é¢˜
        """
        for attempt in range(self.max_retries + 1):
            try:
                return func(*args, **kwargs)
            except DatabaseError as e:
                if "database is locked" in str(e).lower() and attempt < self.max_retries:
                    # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼Œé¿å…æƒŠç¾¤æ•ˆåº”
                    delay = self.base_delay * (2 ** attempt) + random.uniform(0, 0.1)
                    logger.warning(f"ğŸ”„ æ•°æ®åº“é”å®šï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.2f}s: {str(e)}")
                    time.sleep(delay)
                    continue
                else:
                    # é‡è¯•æ¬¡æ•°ç”¨å®Œæˆ–ä¸æ˜¯é”å®šé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                    raise
            except Exception as e:
                # éæ•°æ®åº“é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                raise

    @transaction.atomic
    def create_analysis_safely(self, media, model, analysis_options, prompt=None):
        """åŸå­æ€§åˆ›å»ºåˆ†æä»»åŠ¡"""
        from ..models import OllamaImageAnalysis

        # ä½¿ç”¨ select_for_update é˜²æ­¢ç«æ€æ¡ä»¶
        media_lock = media.__class__.objects.select_for_update().get(id=media.id)

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿›è¡Œä¸­çš„ä»»åŠ¡ï¼ˆå…è®¸é‡å¤åˆ†æå·²å®Œæˆä»»åŠ¡ï¼‰
        existing_analysis = OllamaImageAnalysis.objects.filter(
            media=media_lock,
            model=model,
            analysis_options=analysis_options,
            status__in=['pending', 'processing']  # åªæ£€æŸ¥è¿›è¡Œä¸­çš„ä»»åŠ¡
        ).select_for_update().first()

        if existing_analysis:
            logger.info(f"å‘ç°å·²æœ‰è¿›è¡Œä¸­çš„åˆ†æä»»åŠ¡: {existing_analysis.id}")
            return existing_analysis, False  # è¿”å›ç°æœ‰ä»»åŠ¡ï¼Œ Falseè¡¨ç¤ºæœªåˆ›å»ºæ–°ä»»åŠ¡

        # åŸå­æ€§åˆ›å»ºæ–°ä»»åŠ¡
        analysis = OllamaImageAnalysis.objects.create(
            media=media_lock,
            model=model,
            analysis_options=analysis_options,
            prompt=prompt,
            status='pending'  # ç¡®ä¿åˆå§‹çŠ¶æ€æ­£ç¡®
        )

        logger.info(f"âœ… åŸå­æ€§åˆ›å»ºåˆ†æä»»åŠ¡: {analysis.id}")
        return analysis, True  # è¿”å›æ–°åˆ›å»ºçš„ä»»åŠ¡ï¼Œ Trueè¡¨ç¤ºåˆ›å»ºäº†æ–°ä»»åŠ¡

    def update_analysis_status(self, analysis_id: int, from_status: Optional[str], to_status: str, **kwargs) -> bool:
        """åŸå­æ€§æ›´æ–°åˆ†æçŠ¶æ€ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        from ..models import OllamaImageAnalysis

        def _do_update():
            with transaction.atomic():
                # ä½¿ç”¨ select_for_update é”å®šè®°å½•é¿å…æ­»é”
                analysis = OllamaImageAnalysis.objects.select_for_update(skip_locked=False).get(id=analysis_id)

                # å¢å¼ºçŠ¶æ€æ£€æŸ¥é€»è¾‘
                current_status = analysis.status
                
                # å¦‚æœæŒ‡å®šäº†æºçŠ¶æ€ï¼Œæ£€æŸ¥å½“å‰çŠ¶æ€æ˜¯å¦åŒ¹é…
                if from_status:
                    if isinstance(from_status, str):
                        if current_status != from_status:
                            logger.warning(f"çŠ¶æ€ä¸åŒ¹é…: analysis_id={analysis_id}, "
                                         f"current={current_status}, expected={from_status}")
                            return False
                    elif isinstance(from_status, list):
                        if current_status not in from_status:
                            logger.warning(f"çŠ¶æ€ä¸åœ¨é¢„æœŸèŒƒå›´å†…: analysis_id={analysis_id}, "
                                         f"current={current_status}, expected={from_status}")
                            return False

                # æ›´ä¸¥æ ¼çš„çŠ¶æ€è½¬æ¢éªŒè¯
                if not self._is_valid_status_transition(current_status, to_status):
                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœç›®æ ‡çŠ¶æ€æ˜¯cancelledæˆ–failedï¼Œå…è®¸ä»ä»»ä½•çŠ¶æ€è½¬æ¢
                    if to_status not in ['cancelled', 'failed']:
                        logger.error(f"æ— æ•ˆçš„çŠ¶æ€è½¬æ¢: analysis_id={analysis_id}, "
                                   f"{current_status} -> {to_status}")
                        return False
                    else:
                        logger.warning(f"å¼ºåˆ¶çŠ¶æ€è½¬æ¢ï¼ˆå–æ¶ˆ/å¤±è´¥ï¼‰: analysis_id={analysis_id}, "
                                     f"{current_status} -> {to_status}")

                # æ›´æ–°çŠ¶æ€å’Œé™„åŠ å­—æ®µ
                old_status = analysis.status
                analysis.status = to_status

                # æ›´æ–°æ—¶é—´æˆ³
                current_time = timezone.now()
                if to_status == 'processing' and not analysis.started_at:
                    analysis.started_at = current_time
                elif to_status in ['completed', 'failed', 'cancelled'] and not analysis.completed_at:
                    analysis.completed_at = current_time

                # æ›´æ–°å…¶ä»–å­—æ®µ
                for key, value in kwargs.items():
                    if hasattr(analysis, key):
                        setattr(analysis, key, value)

                # è®¡ç®—å¤„ç†æ—¶é—´
                if to_status == 'completed':
                    processing_time_ms = kwargs.get('processing_time')
                    if processing_time_ms is not None:
                        analysis.processing_time = processing_time_ms
                        logger.debug(f"ä½¿ç”¨ä¼ å…¥çš„å¤„ç†æ—¶é—´: {processing_time_ms}ms, analysis_id={analysis_id}")
                    elif analysis.started_at:
                        calculated_time = int((current_time - analysis.started_at).total_seconds() * 1000)
                        analysis.processing_time = calculated_time
                        logger.debug(f"è®¡ç®—çš„å¤„ç†æ—¶é—´: {calculated_time}ms, analysis_id={analysis_id}")
                    else:
                        analysis.processing_time = 0
                        logger.warning(f"æ— æ³•è®¡ç®—å¤„ç†æ—¶é—´ï¼Œstarted_atä¸ºç©ºï¼Œè®¾ç½®ä¸º0, analysis_id={analysis_id}")

                # ç¡®ä¿å¤„ç†æ—¶é—´ä¸ä¸ºNone
                if to_status == 'completed' and not analysis.processing_time:
                    analysis.processing_time = int((current_time - analysis.started_at).total_seconds() * 1000) if analysis.started_at else 0
                    logger.warning(f"å¤„ç†æ—¶é—´ä¸ºç©ºï¼Œé‡æ–°è®¡ç®—: {analysis.processing_time}ms, analysis_id={analysis_id}")

                analysis.save()

                # æ¸…é™¤ç›¸å…³ç¼“å­˜
                self._clear_analysis_cache(analysis_id)

                logger.info(f"âœ… çŠ¶æ€æ›´æ–°æˆåŠŸ: analysis_id={analysis_id}, "
                           f"{old_status} -> {to_status}")
                return True

        try:
            return self._retry_with_backoff(_do_update)
        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€æ›´æ–°å¤±è´¥: analysis_id={analysis_id}, error={str(e)}")
            return False

    def batch_update_status(self, analysis_ids: List[int], from_status: Optional[str], to_status: str, **kwargs) -> Dict[str, int]:
        """æ‰¹é‡åŸå­æ€§æ›´æ–°çŠ¶æ€ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        from ..models import OllamaImageAnalysis

        def _do_batch_update():
            with transaction.atomic():
                # ä¼˜åŒ–ï¼šå…ˆè·å–å½“å‰çŠ¶æ€ï¼Œç”¨äºæ—¥å¿—è®°å½•
                current_statuses = dict(
                    OllamaImageAnalysis.objects.filter(id__in=analysis_ids)
                    .values_list('id', 'status')
                )
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                queryset = OllamaImageAnalysis.objects.filter(id__in=analysis_ids)

                if from_status:
                    if isinstance(from_status, str):
                        queryset = queryset.filter(status=from_status)
                    elif isinstance(from_status, list):
                        queryset = queryset.filter(status__in=from_status)

                # æ›´çµæ´»çš„çŠ¶æ€è½¬æ¢é€»è¾‘
                if to_status == 'cancelled':
                    # å…è®¸ä» pending æˆ– processing çŠ¶æ€å–æ¶ˆ
                    if not from_status:  # å¦‚æœæ²¡æœ‰æŒ‡å®šæºçŠ¶æ€ï¼Œåˆ™è¿‡æ»¤
                        queryset = queryset.filter(status__in=['pending', 'processing'])
                elif to_status == 'processing':
                    # åªèƒ½ä» pending çŠ¶æ€å¼€å§‹å¤„ç†
                    if not from_status:  # å¦‚æœæ²¡æœ‰æŒ‡å®šæºçŠ¶æ€ï¼Œåˆ™è¿‡æ»¤
                        queryset = queryset.filter(status='pending')
                elif to_status == 'failed':
                    # å¯ä»¥ä»ä»»ä½•çŠ¶æ€æ ‡è®°ä¸ºå¤±è´¥ï¼Œä¸é¢å¤–è¿‡æ»¤
                    pass
                elif to_status == 'completed':
                    # åªèƒ½ä» processing çŠ¶æ€å®Œæˆ
                    if not from_status:  # å¦‚æœæ²¡æœ‰æŒ‡å®šæºçŠ¶æ€ï¼Œåˆ™è¿‡æ»¤
                        queryset = queryset.filter(status='processing')

                # å‡†å¤‡æ›´æ–°æ•°æ®
                update_data = {'status': to_status}
                current_time = timezone.now()

                if to_status == 'processing':
                    update_data['started_at'] = current_time
                elif to_status in ['completed', 'failed', 'cancelled']:
                    update_data['completed_at'] = current_time

                # æ·»åŠ å…¶ä»–æ›´æ–°å­—æ®µ
                update_data.update(kwargs)

                # æ‰§è¡Œæ‰¹é‡æ›´æ–°
                updated_count = queryset.update(**update_data)

                # ä¼˜åŒ–ï¼šè®°å½•è¯¦ç»†çš„çŠ¶æ€è½¬æ¢ä¿¡æ¯
                if updated_count < len(analysis_ids):
                    # æ‰¾å‡ºæœªæ›´æ–°çš„è®°å½• - ä¿®å¤ï¼šåº”è¯¥æ£€æŸ¥å“ªäº›è®°å½•å®é™…è¢«æ›´æ–°äº†
                    actually_updated_ids = set(
                        OllamaImageAnalysis.objects.filter(id__in=analysis_ids, status=to_status)
                        .values_list('id', flat=True)
                    )
                    not_updated_ids = set(analysis_ids) - actually_updated_ids
                    
                    for analysis_id in not_updated_ids:
                        current_status = current_statuses.get(analysis_id, 'unknown')
                        logger.warning(f"æ‰¹é‡æ›´æ–°è·³è¿‡: analysis_id={analysis_id}, "
                                     f"current_status={current_status}, target_status={to_status}")

                # æ¸…é™¤ç¼“å­˜
                for analysis_id in analysis_ids:
                    self._clear_analysis_cache(analysis_id)

                logger.info(f"âœ… æ‰¹é‡çŠ¶æ€æ›´æ–°å®Œæˆ: æˆåŠŸ {updated_count}/{len(analysis_ids)} ä¸ª, "
                           f"çŠ¶æ€: {from_status or '*'} -> {to_status}")

                return {
                    'success_count': updated_count,
                    'error_count': len(analysis_ids) - updated_count
                }

        try:
            return self._retry_with_backoff(_do_batch_update)
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡çŠ¶æ€æ›´æ–°å¤±è´¥: error={str(e)}")
            return {'success_count': 0, 'error_count': len(analysis_ids)}

    def _is_valid_status_transition(self, from_status: str, to_status: str) -> bool:
        """éªŒè¯çŠ¶æ€è½¬æ¢æ˜¯å¦æœ‰æ•ˆ"""
        valid_transitions = {
            'pending': ['processing', 'cancelled', 'failed'],
            'processing': ['completed', 'failed', 'cancelled'],
            'completed': [],  # å·²å®Œæˆä¸èƒ½è½¬æ¢
            'failed': ['pending'],  # å¤±è´¥å¯ä»¥é‡è¯•
            'cancelled': []  # å·²å–æ¶ˆä¸èƒ½è½¬æ¢
        }

        # ä¼˜åŒ–ï¼šå…è®¸ç‰¹æ®Šæƒ…å†µä¸‹çš„çŠ¶æ€è½¬æ¢
        # å¦‚æœç›®æ ‡çŠ¶æ€æ˜¯cancelledæˆ–failedï¼Œå…è®¸ä»ä»»ä½•çŠ¶æ€è½¬æ¢ï¼ˆç”¨äºå¼ºåˆ¶å–æ¶ˆæˆ–æ ‡è®°å¤±è´¥ï¼‰
        if to_status in ['cancelled', 'failed']:
            return True
            
        return to_status in valid_transitions.get(from_status, [])

    def _clear_analysis_cache(self, analysis_id: int):
        """æ¸…é™¤åˆ†æä»»åŠ¡ç›¸å…³ç¼“å­˜"""
        cache_keys = [
            f'analysis_status_{analysis_id}',
            f'analysis_details_{analysis_id}',
            f'user_task_counts_*',  # ç”¨æˆ·ä»»åŠ¡ç»Ÿè®¡ç¼“å­˜
        ]

        for key in cache_keys:
            if key.endswith('*'):
                # æ¨¡ç³ŠåŒ¹é…åˆ é™¤ - Djangoç¼“å­˜å¯èƒ½ä¸æ”¯æŒkeysæ–¹æ³•ï¼Œä½¿ç”¨ç®€å•çš„åˆ é™¤
                try:
                    cache.delete(key.replace('*', ''))
                except:
                    pass  # å¿½ç•¥åˆ é™¤å¤±è´¥
            else:
                cache.delete(key)

    @transaction.atomic
    def increment_retry_count(self, analysis_id: int) -> bool:
        """åŸå­æ€§å¢åŠ é‡è¯•æ¬¡æ•°"""
        from ..models import OllamaImageAnalysis

        try:
            with transaction.atomic():
                analysis = OllamaImageAnalysis.objects.select_for_update().get(id=analysis_id)

                if analysis.retry_count >= 3:  # æœ€å¤§é‡è¯•æ¬¡æ•°é™åˆ¶
                    logger.warning(f"åˆ†æä»»åŠ¡å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: analysis_id={analysis_id}")
                    return False

                analysis.retry_count = F('retry_count') + 1
                analysis.save()

                # é‡æ–°è·å–å¯¹è±¡ä»¥è·å–æ›´æ–°åçš„å€¼
                analysis.refresh_from_db()
                logger.info(f"ğŸ”„ é‡è¯•æ¬¡æ•°æ›´æ–°: analysis_id={analysis_id}, retry_count={analysis.retry_count}")
                return True

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°é‡è¯•æ¬¡æ•°å¤±è´¥: analysis_id={analysis_id}, error={str(e)}")
            return False

    def update_media_with_analysis_result(self, analysis, result: Dict[str, Any]) -> bool:
        """åŸå­æ€§æ›´æ–°åª’ä½“åˆ†æç»“æœï¼ˆä½¿ç”¨é‡è¯•æœºåˆ¶ï¼‰"""
        from media.models import Media, Category, Tag

        def _do_update():
            with transaction.atomic():
                # é”å®šåª’ä½“è®°å½•
                media = Media.objects.select_for_update().get(id=analysis.media.id)

                # æ›´æ–°åª’ä½“å­—æ®µ
                if result.get('title'):
                    media.title = result['title'][:200]  # é™åˆ¶é•¿åº¦

                if result.get('description'):
                    media.description = result['description'][:1000]  # é™åˆ¶é•¿åº¦

                if result.get('prompt'):
                    media.prompt = result['prompt'][:500]  # é™åˆ¶é•¿åº¦

                # å¤„ç†åˆ†ç±»
                if result.get('categories') and isinstance(result['categories'], list):
                    # è·å–æˆ–åˆ›å»ºåˆ†ç±»å¯¹è±¡
                    category_objects = []
                    for cat_name in result['categories'][:10]:  # é™åˆ¶æ•°é‡
                        if isinstance(cat_name, str):
                            category, created = Category.objects.get_or_create(
                                name=cat_name[:100],  # é™åˆ¶é•¿åº¦
                                user=media.user,
                                defaults={'description': f'è‡ªåŠ¨ç”Ÿæˆçš„åˆ†ç±»: {cat_name}'}
                            )
                            category_objects.append(category)
                    
                    # è®¾ç½®åˆ†ç±»å…³ç³»
                    media.categories.set(category_objects)

                # å¤„ç†æ ‡ç­¾
                if result.get('tags') and isinstance(result['tags'], list):
                    # è·å–æˆ–åˆ›å»ºæ ‡ç­¾å¯¹è±¡
                    tag_objects = []
                    for tag_name in result['tags'][:20]:  # é™åˆ¶æ•°é‡
                        if isinstance(tag_name, str):
                            tag, created = Tag.objects.get_or_create(
                                name=tag_name[:50],  # é™åˆ¶é•¿åº¦
                                user=media.user,
                                defaults={}
                            )
                            tag_objects.append(tag)
                    
                    # è®¾ç½®æ ‡ç­¾å…³ç³»
                    media.tags.set(tag_objects)

                # æ›´æ–°æ—¶é—´æˆ³
                media.save()

                logger.info(f"âœ… åª’ä½“åˆ†æç»“æœæ›´æ–°æˆåŠŸ: media_id={media.id}")
                return True

        try:
            return self._retry_with_backoff(_do_update)
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°åª’ä½“åˆ†æç»“æœå¤±è´¥: media_id={analysis.media.id}, error={str(e)}")
            return False

    def get_user_task_statistics(self, user_id: int) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ä»»åŠ¡ç»Ÿè®¡ï¼ˆå®æ—¶æ•°æ®ï¼‰"""
        try:
            from ..models import OllamaImageAnalysis

            # ä½¿ç”¨èšåˆæŸ¥è¯¢è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = OllamaImageAnalysis.objects.filter(
                media__user_id=user_id
            ).values('status').annotate(
                count=models.Count('id')
            )

            # æ„å»ºç»Ÿè®¡å­—å…¸
            result = {
                'pending': 0,
                'processing': 0,
                'completed': 0,
                'failed': 0,
                'cancelled': 0,
                'total': 0
            }

            for stat in stats:
                status = stat['status']
                count = stat['count']
                if status in result:
                    result[status] = count
                result['total'] += count

            # æ·»åŠ å…¶ä»–ç»Ÿè®¡ä¿¡æ¯
            result['processing_time_avg'] = self._get_avg_processing_time(user_id)
            result['last_activity'] = self._get_last_activity_time(user_id)

            return result

        except Exception as e:
            logger.error(f"âŒ è·å–ç”¨æˆ·ç»Ÿè®¡å¤±è´¥: user_id={user_id}, error={str(e)}")
            return {}

    def _get_avg_processing_time(self, user_id: int) -> float:
        """è·å–å¹³å‡å¤„ç†æ—¶é—´"""
        try:
            from ..models import OllamaImageAnalysis

            avg_time = OllamaImageAnalysis.objects.filter(
                media__user_id=user_id,
                status='completed',
                processing_time__isnull=False
            ).aggregate(
                avg_time=models.Avg('processing_time')
            )['avg_time']

            return round(avg_time or 0, 2)
        except:
            return 0.0

    def _get_last_activity_time(self, user_id: int) -> Optional[timezone.datetime]:
        """è·å–æœ€åæ´»åŠ¨æ—¶é—´"""
        try:
            from ..models import OllamaImageAnalysis

            last_analysis = OllamaImageAnalysis.objects.filter(
                media__user_id=user_id
            ).order_by('-created_at').first()

            return last_analysis.created_at if last_analysis else None
        except:
            return None

    @transaction.atomic
    def cleanup_old_analyses(self, days_old: int = 30) -> Dict[str, int]:
        """æ¸…ç†æ—§çš„åˆ†æè®°å½•"""
        from ..models import OllamaImageAnalysis

        try:
            cutoff_date = timezone.now() - timezone.timedelta(days=days_old)

            with transaction.atomic():
                # è·å–è¦åˆ é™¤çš„è®°å½•æ•°é‡
                old_count = OllamaImageAnalysis.objects.filter(
                    created_at__lt=cutoff_date,
                    status__in=['completed', 'failed', 'cancelled']
                ).count()

                if old_count == 0:
                    logger.info(f"æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§åˆ†æè®°å½•ï¼ˆ{days_old}å¤©å‰ï¼‰")
                    return {'deleted_count': 0}

                # åˆ é™¤æ—§è®°å½•
                deleted_count, _ = OllamaImageAnalysis.objects.filter(
                    created_at__lt=cutoff_date,
                    status__in=['completed', 'failed', 'cancelled']
                ).delete()

                logger.info(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted_count} ä¸ªæ—§åˆ†æè®°å½•")
                return {'deleted_count': deleted_count}

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§åˆ†æè®°å½•å¤±è´¥: error={str(e)}")
            return {'deleted_count': 0, 'error': str(e)}


# å…¨å±€çŠ¶æ€ç®¡ç†å™¨å®ä¾‹
state_manager = StateManager()