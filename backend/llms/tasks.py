"""
Django Async Manager ä»»åŠ¡å¤„ç†å™¨
ç”¨äºå¼‚æ­¥å¤„ç†å›¾ç‰‡åˆ†æä»»åŠ¡
"""
import logging
import requests
import base64
from typing import Dict, Any, Optional
from django.utils import timezone
from django.db import transaction
from django_async_manager import get_background_task

background_task = get_background_task()
from .models import AIAnalysis, AIModel, OllamaEndpoint
from media.models import Media, Category, Tag

logger = logging.getLogger(__name__)


@background_task(max_retries=3, retry_delay=60)
def analyze_image(analysis_id: int) -> Dict[str, Any]:
    """
    åˆ†æå›¾ç‰‡çš„ä¸»ä»»åŠ¡å‡½æ•°
    è¿™ä¸ªå‡½æ•°ä¼šè¢« django-async-manager å¼‚æ­¥æ‰§è¡Œ
    """
    logger.info(f"ğŸš€ [TASK-{analysis_id}] å¼€å§‹å¤„ç†å›¾ç‰‡åˆ†æä»»åŠ¡")

    try:
        # è·å–åˆ†æè®°å½•å¹¶ç«‹å³é”å®šæ¨¡å‹åç§°
        with transaction.atomic():
            analysis = AIAnalysis.objects.select_for_update().select_related('media').get(id=analysis_id)
            initial_model = analysis.model_used
            logger.info(f"ğŸ“‹ [TASK-{analysis_id}] è·å–åˆ†æè®°å½•æˆåŠŸ: ID={analysis.id}, åª’ä½“æ–‡ä»¶={analysis.media.file.name}")
            logger.info(f"ğŸ¯ [TASK-{analysis_id}] é”å®šçš„åˆå§‹æ¨¡å‹åç§°: {initial_model}")

            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­ï¼Œä½†ä¸æ”¹å˜model_usedå­—æ®µ
            analysis.status = 'processing'
            analysis.save(update_fields=['status'])
            logger.info(f"â³ [TASK-{analysis_id}] æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤„ç†ä¸­")

        # è·å–åª’ä½“æ–‡ä»¶
        media = analysis.media
        logger.info(f"ğŸ“ [TASK-{analysis_id}] åª’ä½“æ–‡ä»¶ä¿¡æ¯: ID={media.id}, æ–‡ä»¶å={media.file.name}, ç±»å‹={media.file_type}, å¤§å°={media.file.size if media.file else 'Unknown'} bytes")

        # éªŒè¯æ–‡ä»¶ç±»å‹
        if media.file_type != 'image':
            logger.error(f"âŒ [TASK-{analysis_id}] æ–‡ä»¶ç±»å‹ä¸æ”¯æŒ: {media.file_type}")
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {media.file_type}")

        # è·å–æŒ‡å®šçš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤è§†è§‰æ¨¡å‹
        model_name = initial_model  # ä½¿ç”¨é”å®šçš„åˆå§‹æ¨¡å‹åç§°
        logger.info(f"ğŸ” [TASK-{analysis_id}] ä½¿ç”¨é”å®šçš„æ¨¡å‹åç§°: {model_name}")
        
        if model_name:
            try:
                model = AIModel.objects.get(name=model_name, is_active=True, is_vision_capable=True)
                logger.info(f"ğŸ¯ [TASK-{analysis_id}] æˆåŠŸè·å–æŒ‡å®šæ¨¡å‹: {model_name} (ID: {model.id})")
            except AIModel.DoesNotExist:
                logger.warning(f"âš ï¸ [TASK-{analysis_id}] æŒ‡å®šæ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨: {model_name}ï¼Œå›é€€åˆ°é»˜è®¤æ¨¡å‹")
                model = AIModel.get_default_model()
                if not model:
                    logger.error(f"âŒ [TASK-{analysis_id}] æœªæ‰¾åˆ°å¯ç”¨çš„AIè§†è§‰æ¨¡å‹")
                    raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„AIè§†è§‰æ¨¡å‹")
                logger.info(f"ğŸ”„ [TASK-{analysis_id}] å›é€€åˆ°é»˜è®¤æ¨¡å‹: {model.name}")
                # é‡è¦ï¼šå³ä½¿å›é€€åˆ°é»˜è®¤æ¨¡å‹ï¼Œä¹Ÿè¦ä¿æŒåŸå§‹æ¨¡å‹åç§°è®°å½•
                logger.warning(f"âš ï¸ [TASK-{analysis_id}] æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ¨¡å‹ {model.name}ï¼Œä½†è®°å½•ä¸­ä¿æŒåŸå§‹æ¨¡å‹åç§° {model_name}")
        else:
            logger.warning(f"âš ï¸ [TASK-{analysis_id}] åˆ†æè®°å½•ä¸­æ²¡æœ‰ä¿å­˜æ¨¡å‹åç§°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            model = AIModel.get_default_model()
            if not model:
                logger.error(f"âŒ [TASK-{analysis_id}] æœªæ‰¾åˆ°å¯ç”¨çš„AIè§†è§‰æ¨¡å‹")
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„AIè§†è§‰æ¨¡å‹")
            logger.info(f"ğŸ”„ [TASK-{analysis_id}] ä½¿ç”¨é»˜è®¤æ¨¡å‹: {model.name}")
            # æ›´æ–°è®°å½•ä¸­çš„æ¨¡å‹åç§°ä¸ºå®é™…ä½¿ç”¨çš„é»˜è®¤æ¨¡å‹
            with transaction.atomic():
                analysis_with_lock = AIAnalysis.objects.select_for_update().get(id=analysis_id)
                analysis_with_lock.model_used = model.name
                analysis_with_lock.save(update_fields=['model_used'])
            model_name = model.name

        # è·å–ç«¯ç‚¹
        endpoint = model.endpoint
        logger.info(f"ğŸ¤– [TASK-{analysis_id}] AIæ¨¡å‹ä¿¡æ¯: æ¨¡å‹={model.name}, ç«¯ç‚¹={endpoint.url}, ç«¯ç‚¹åç§°={endpoint.name}")

        # è¯»å–å›¾ç‰‡æ–‡ä»¶å¹¶ç¼–ç 
        logger.info(f"ğŸ–¼ï¸ [TASK-{analysis_id}] å¼€å§‹è¯»å–å›¾ç‰‡æ–‡ä»¶: {media.file.path}")
        image_data = _encode_image(media.file.path)
        logger.info(f"âœ… [TASK-{analysis_id}] å›¾ç‰‡æ–‡ä»¶ç¼–ç å®Œæˆï¼ŒBase64é•¿åº¦: {len(image_data)} å­—ç¬¦")

        # è·å–åˆ†æé€‰é¡¹
        options = analysis.analysis_options or {}
        logger.info(f"âš™ï¸ [TASK-{analysis_id}] åˆ†æé€‰é¡¹é…ç½®: {options}")

        # è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ
        logger.info(f"ğŸ“¡ [TASK-{analysis_id}] å¼€å§‹è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ...")
        start_time = timezone.now()

        result = _call_ollama_api(
            endpoint_url=endpoint.url,
            model_name=model.name,
            image_data=image_data,
            media_file=media,
            options=options
        )

        end_time = timezone.now()
        duration = (end_time - start_time).total_seconds()
        logger.info(f"ğŸ“¡ [TASK-{analysis_id}] AIæ¨¡å‹è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")

        # æ£€æŸ¥å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
        logger.info(f"ğŸ” [TASK-{analysis_id}] å®é™…ä½¿ç”¨çš„æ¨¡å‹: {model.name}")

        # è®°å½•åˆ†æç»“æœæ¦‚è¦
        if result:
            title = result.get('title', '')[:50]
            description_length = len(result.get('description', ''))
            tags_count = len(result.get('tags', []))
            categories_count = len(result.get('categories', []))
            logger.info(f"ğŸ“Š [TASK-{analysis_id}] åˆ†æç»“æœæ¦‚è¦: æ ‡é¢˜='{title}...', æè¿°é•¿åº¦={description_length}, æ ‡ç­¾æ•°é‡={tags_count}, åˆ†ç±»æ•°é‡={categories_count}")
        else:
            logger.warning(f"âš ï¸ [TASK-{analysis_id}] AIè¿”å›ç©ºç»“æœ")

        # å¤„ç†åˆ†æç»“æœ
        logger.info(f"ğŸ’¾ [TASK-{analysis_id}] å¼€å§‹ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“...")

        # ä½¿ç”¨é”å®šçš„åŸå§‹æ¨¡å‹åç§°
        original_model_name = initial_model
        logger.info(f"ğŸ”’ [TASK-{analysis_id}] ä½¿ç”¨é”å®šçš„åŸå§‹æ¨¡å‹åç§°: {original_model_name}")

        # ä¼ é€’åŸå§‹æ¨¡å‹åç§°ç»™å¤„ç†å‡½æ•°ï¼Œç¡®ä¿ä¸ä¼šè¢«è¦†ç›–
        _process_analysis_result(analysis, result, original_model_name)

        # æœ€ç»ˆéªŒè¯
        final_check = AIAnalysis.objects.get(id=analysis_id)
        if final_check.model_used != original_model_name:
            logger.error(f"âŒ [TASK-{analysis_id}] æœ€ç»ˆéªŒè¯å¤±è´¥ï¼æœŸæœ›: {original_model_name}, å®é™…: {final_check.model_used}")
            # ä½¿ç”¨åŸç”ŸSQLå¼ºåˆ¶æ›´æ–°
            from django.db import connection
            with connection.cursor() as cursor:
                cursor.execute(
                    "UPDATE llms_aianalysis SET model_used = %s WHERE id = %s",
                    [original_model_name, analysis_id]
                )
            logger.info(f"ğŸ”§ [TASK-{analysis_id}] ä½¿ç”¨åŸç”ŸSQLå¼ºåˆ¶ä¿®å¤æ¨¡å‹åç§°")
        else:
            logger.info(f"âœ… [TASK-{analysis_id}] æ¨¡å‹åç§°éªŒè¯é€šè¿‡: {original_model_name}")

        logger.info(f"âœ… [TASK-{analysis_id}] åˆ†æç»“æœä¿å­˜å®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹: {original_model_name}")

        logger.info(f"ğŸ‰ [TASK-{analysis_id}] å›¾ç‰‡åˆ†æä»»åŠ¡å®ŒæˆæˆåŠŸ")

        return {
            'success': True,
            'analysis_id': analysis_id,
            'result': result
        }

    except Exception as e:
        logger.error(f"âŒ [TASK-{analysis_id}] å›¾ç‰‡åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        logger.error(f"âŒ [TASK-{analysis_id}] é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}", exc_info=True)

        # æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤±è´¥
        try:
            analysis = AIAnalysis.objects.get(id=analysis_id)
            analysis.status = 'failed'
            analysis.error_message = str(e)
            analysis.analyzed_at = timezone.now()
            analysis.save()
            logger.info(f"ğŸ“ [TASK-{analysis_id}] å·²æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤±è´¥")
        except Exception as save_error:
            logger.error(f"âŒ [TASK-{analysis_id}] æ›´æ–°åˆ†æçŠ¶æ€å¤±è´¥: {save_error}")

        logger.info(f"ğŸ [TASK-{analysis_id}] ä»»åŠ¡æ‰§è¡Œç»“æŸ (å¤±è´¥)")

        return {
            'success': False,
            'analysis_id': analysis_id,
            'error': str(e)
        }


def _encode_image(image_path: str) -> str:
    """å°†å›¾ç‰‡æ–‡ä»¶ç¼–ç ä¸ºbase64"""
    try:
        logger.debug(f"ğŸ“– [ENCODE] å¼€å§‹è¯»å–å›¾ç‰‡æ–‡ä»¶: {image_path}")
        with open(image_path, 'rb') as image_file:
            image_bytes = image_file.read()
            encoded_data = base64.b64encode(image_bytes).decode('utf-8')
            logger.debug(f"ğŸ“– [ENCODE] å›¾ç‰‡ç¼–ç å®Œæˆ: åŸå§‹å¤§å°={len(image_bytes)} bytes, ç¼–ç å¤§å°={len(encoded_data)} å­—ç¬¦")
            return encoded_data
    except Exception as e:
        logger.error(f"âŒ [ENCODE] å›¾ç‰‡æ–‡ä»¶è¯»å–å¤±è´¥: {image_path}, é”™è¯¯: {str(e)}")
        raise ValueError(f"æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶: {str(e)}")


def _call_ollama_api(endpoint_url: str, model_name: str, image_data: str, media_file: Media, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """è°ƒç”¨Ollama APIè¿›è¡Œå›¾ç‰‡åˆ†æ"""
    timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶

    logger.debug(f"ğŸ”§ [API] å¼€å§‹æ„å»ºAPIè¯·æ±‚å‚æ•°")

    # æ„å»ºåˆ†ææç¤ºè¯
    prompt = _build_analysis_prompt(media_file, options)
    logger.debug(f"ğŸ’¬ [API] æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

    # API è¯·æ±‚æ•°æ®
    payload = {
        "model": model_name,
        "prompt": prompt,
        "images": [image_data],
        "stream": False
    }

    api_url = f"{endpoint_url.rstrip('/')}/api/generate"
    logger.info(f"ğŸŒ [API] å‡†å¤‡è°ƒç”¨Ollama API: {api_url}")
    logger.debug(f"ğŸŒ [API] è¯·æ±‚å‚æ•°: æ¨¡å‹={model_name}, è¶…æ—¶={timeout}ç§’")

    try:
        logger.debug(f"ğŸ“¤ [API] å‘é€HTTPè¯·æ±‚...")
        response = requests.post(
            api_url,
            json=payload,
            timeout=timeout
        )

        logger.info(f"ğŸ“¤ [API] APIå“åº”: çŠ¶æ€ç ={response.status_code}, å“åº”å¤§å°={len(response.content)} bytes")

        # è®°å½•å®Œæ•´å“åº”ç”¨äºè°ƒè¯•
        result = response.json()
        logger.debug(f"ğŸ“¤ [API] åŸå§‹å“åº”æ•°æ®: {result}")

        # æ£€æŸ¥å“åº”ç»“æ„
        if 'response' in result:
            logger.info(f"ğŸ“¤ [API] æ‰¾åˆ°AIå“åº”å­—æ®µï¼Œå“åº”é•¿åº¦: {len(result.get('response', ''))} å­—ç¬¦")
            logger.debug(f"ğŸ“¤ [API] AIå“åº”å‰300å­—ç¬¦: {result.get('response', '')[:300]}")
        else:
            logger.warning(f"ğŸ“¤ [API] å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°'response'å­—æ®µï¼Œå“åº”ç»“æ„: {list(result.keys())}")

        # è§£æAIå“åº”
        parsed_result = _parse_ai_response(result.get('response', ''))
        logger.info(f"ğŸ“¤ [API] AIå“åº”è§£æå®Œæˆï¼Œç»“æœç±»å‹: {type(parsed_result)}")
        logger.debug(f"ğŸ“¤ [API] è§£æç»“æœæ¦‚è¦: {list(parsed_result.keys()) if parsed_result else 'None'}")

        return parsed_result

    except requests.exceptions.Timeout as e:
        logger.error(f"â° [API] è¯·æ±‚è¶…æ—¶ ({timeout}ç§’): {str(e)}")
        raise ValueError(f"AIæ¨¡å‹è°ƒç”¨è¶…æ—¶: {str(e)}")
    except requests.exceptions.ConnectionError as e:
        logger.error(f"ğŸ”Œ [API] è¿æ¥é”™è¯¯: {str(e)}")
        raise ValueError(f"æ— æ³•è¿æ¥åˆ°AIæ¨¡å‹æœåŠ¡: {str(e)}")
    except requests.exceptions.HTTPError as e:
        logger.error(f"ğŸŒ [API] HTTPé”™è¯¯: {str(e)}")
        raise ValueError(f"AIæ¨¡å‹è°ƒç”¨HTTPé”™è¯¯: {str(e)}")
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ [API] è¯·æ±‚å¼‚å¸¸: {str(e)}")
        raise ValueError(f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
    except Exception as e:
        logger.error(f"âŒ [API] æœªçŸ¥é”™è¯¯: {str(e)}")
        raise ValueError(f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")


def _build_prompt_only_prompt(media_file: Media, analysis_result: Dict[str, Any]) -> str:
    """æ„å»ºä¸“é—¨ç”¨äºç”Ÿæˆæç¤ºè¯çš„æç¤ºè¯"""

    title = analysis_result.get('title', '')
    description = analysis_result.get('description', '')

    prompt = f"""åŸºäºä»¥ä¸‹å›¾ç‰‡åˆ†æç»“æœï¼Œè¯·é‡æ–°ç”Ÿæˆä¸€ä¸ªæ›´ä¸“ä¸šçš„AIç»˜ç”»æç¤ºè¯ï¼š

å›¾ç‰‡æ ‡é¢˜ï¼š{title}
å›¾ç‰‡æè¿°ï¼š{description}

è¯·ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ä¸­æ–‡AIç»˜ç”»æç¤ºè¯ï¼Œè¦åŒ…å«ï¼š
1. ä¸»ä½“ç‰¹å¾å’Œç»†èŠ‚
2. ç¯å¢ƒå’ŒèƒŒæ™¯
3. è‰²å½©å’Œå…‰çº¿
4. è‰ºæœ¯é£æ ¼
5. æ°›å›´å’Œæƒ…æ„Ÿ

åªéœ€è¦è¿”å›æç¤ºè¯å†…å®¹ï¼Œä¸éœ€è¦JSONæ ¼å¼ã€‚"""

    return prompt


def _call_ollama_for_prompt_only(endpoint_url: str, model_name: str, image_data: str, prompt: str) -> str:
    """ä¸“é—¨è°ƒç”¨Ollama APIç”Ÿæˆæç¤ºè¯"""
    timeout = 300

    api_url = f"{endpoint_url.rstrip('/')}/api/generate"

    payload = {
        "model": model_name,
        "prompt": prompt,
        "images": [image_data],
        "stream": False
    }

    try:
        logger.info(f"ğŸ¨ [PROMPT] å¼€å§‹ä¸“é—¨ç”Ÿæˆæç¤ºè¯...")
        response = requests.post(api_url, json=payload, timeout=timeout)

        if response.status_code == 200:
            result = response.json()
            prompt_result = result.get('response', '').strip()
            logger.info(f"ğŸ¨ [PROMPT] æç¤ºè¯ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(prompt_result)} å­—ç¬¦")
            return prompt_result
        else:
            logger.error(f"ğŸ¨ [PROMPT] APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            return ""

    except Exception as e:
        logger.error(f"ğŸ¨ [PROMPT] ç”Ÿæˆæç¤ºè¯å¤±è´¥: {str(e)}")
        return ""


def _build_analysis_prompt(media_file: Media, options: Optional[Dict[str, Any]] = None) -> str:
    """æ„å»ºå›¾ç‰‡åˆ†ææç¤ºè¯"""
    # é»˜è®¤é€‰é¡¹
    default_options = {
        'generate_title': True,
        'generate_description': True,
        'generate_prompt': True,
        'generate_categories': True,
        'generate_tags': True,
        'max_categories': 5,
        'max_tags': 10
    }

    # åˆå¹¶ç”¨æˆ·é€‰é¡¹å’Œé»˜è®¤é€‰é¡¹
    if options:
        final_options = {**default_options, **options}
    else:
        final_options = default_options

    # æ„å»ºJSONç»“æ„ç¤ºä¾‹ï¼ˆä½¿ç”¨ä¸­æ–‡è¯´æ˜ï¼‰
    json_structure_example = {}

    if final_options.get('generate_title', True):
        json_structure_example['title'] = "ä¸ºå›¾ç‰‡ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æ ‡é¢˜"

    if final_options.get('generate_description', True):
        json_structure_example['description'] = "ç”¨ä¸­æ–‡è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ã€åœºæ™¯ã€å¯¹è±¡å’Œç‰¹å¾"

    if final_options.get('generate_prompt', True):
        json_structure_example['prompt'] = "ç”Ÿæˆé€‚åˆç”¨äºAIç»˜ç”»çš„ä¸­æ–‡æç¤ºè¯ï¼ŒåŒ…å«ç”»é£ã€æ„å›¾ã€è‰²å½©ç­‰è¦ç´ "

    if final_options.get('generate_tags', True):
        max_tags = final_options.get('max_tags', 10)
        json_structure_example['tags'] = [f"ä¸­æ–‡æ ‡ç­¾{i+1}" for i in range(min(3, max_tags))]

    if final_options.get('generate_categories', True):
        max_categories = final_options.get('max_categories', 5)
        json_structure_example['categories'] = [f"ä¸­æ–‡åˆ†ç±»{i+1}" for i in range(min(2, max_categories))]

    # ç”Ÿæˆæç¤ºè¯
    prompt = f"""åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{json_structure_example}

è¯·ç”¨ä¸­æ–‡æè¿°è¿™å¼ å›¾ç‰‡çš„æç¤ºè¯ï¼ŒåŒ…å«å†…å®¹ã€é£æ ¼ã€è‰²å½©å’Œæ°›å›´ç­‰è¦ç´ ã€‚

å½“å‰æ–‡ä»¶åï¼š{media_file.file.name}
å½“å‰å·²æœ‰æè¿°ï¼š{media_file.description or 'æ— '}

è¯·è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""

    return prompt


def _parse_ai_response(response_text: str) -> Dict[str, Any]:
    """è§£æAIå“åº”æ–‡æœ¬"""
    import json
    import re

    logger.info(f"ğŸ” [PARSE] åŸå§‹AIå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
    logger.debug(f"ğŸ” [PARSE] åŸå§‹AIå“åº”å‰500å­—ç¬¦: {response_text[:500]}")

    try:
        # å°è¯•æå–JSON
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            logger.debug(f"ğŸ” [PARSE] æå–çš„JSONå­—ç¬¦ä¸²: {json_str}")
            parsed_data = json.loads(json_str)
            logger.info(f"ğŸ” [PARSE] JSONè§£ææˆåŠŸ: {parsed_data}")
            return parsed_data
        else:
            logger.warning(f"ğŸ” [PARSE] æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œä½¿ç”¨åŸå§‹å“åº”æ–‡æœ¬")
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œåˆ›å»ºåŸºæœ¬å“åº”
            return {
                'title': response_text[:100] + '...' if len(response_text) > 100 else response_text,
                'description': response_text,
                'prompt': response_text,
                'tags': [],
                'categories': []
            }
    except json.JSONDecodeError as e:
        logger.error(f"ğŸ” [PARSE] JSONè§£æå¤±è´¥: {e}, å“åº”å†…å®¹: {response_text[:200]}")
        # JSONè§£æå¤±è´¥ï¼Œåˆ›å»ºåŸºæœ¬å“åº”
        return {
            'title': response_text[:100] + '...' if len(response_text) > 100 else response_text,
            'description': response_text,
            'prompt': response_text,
            'tags': [],
            'categories': []
        }


def _process_analysis_result(analysis: AIAnalysis, result: Dict[str, Any], original_model_name: str = None):
    """å¤„ç†åˆ†æç»“æœå¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
    try:
        with transaction.atomic():
            logger.info(f"ğŸ’¾ [PROCESS] å¼€å§‹å¤„ç†åˆ†æç»“æœ: analysis_id={analysis.id}")

            # è·å–åˆ†æé€‰é¡¹ä¸­çš„é™åˆ¶è®¾ç½®
            analysis_options = analysis.analysis_options or {}
            max_categories = analysis_options.get('max_categories', 5)
            max_tags = analysis_options.get('max_tags', 10)

            logger.info(f"ğŸ’¾ [PROCESS] åˆ†æé™åˆ¶: max_categories={max_categories}, max_tags={max_tags}")

            # ä¿å­˜åˆ†æç»“æœ
            title = result.get('title', '').strip()
            description = result.get('description', '').strip()
            prompt = result.get('prompt', '').strip()
            tag_names = result.get('tags', [])
            category_names = result.get('categories', [])

            # å¼ºåˆ¶é™åˆ¶æ ‡ç­¾å’Œåˆ†ç±»æ•°é‡
            if len(tag_names) > max_tags:
                logger.warning(f"ğŸ’¾ [PROCESS] æ ‡ç­¾æ•°é‡è¶…é™: {len(tag_names)} > {max_tags}, æˆªå–å‰{max_tags}ä¸ª")
                tag_names = tag_names[:max_tags]

            if len(category_names) > max_categories:
                logger.warning(f"ğŸ’¾ [PROCESS] åˆ†ç±»æ•°é‡è¶…é™: {len(category_names)} > {max_categories}, æˆªå–å‰{max_categories}ä¸ª")
                category_names = category_names[:max_categories]
            
            logger.info(f"ğŸ’¾ [PROCESS] åˆ†æç»“æœæ•°æ®: title='{title}', description_length={len(description)}, prompt_length={len(prompt)}")
            logger.info(f"ğŸ’¾ [PROCESS] æ ‡ç­¾({len(tag_names)}ä¸ª): {tag_names}, åˆ†ç±»({len(category_names)}ä¸ª): {category_names}")
            logger.info(f"ğŸ’¾ [PROCESS] åŸå§‹æ¨¡å‹åç§°: {original_model_name}, å½“å‰è®°å½•æ¨¡å‹åç§°: {analysis.model_used}")
            
            # ä½¿ç”¨select_for_updateé”å®šè®°å½•ï¼Œé˜²æ­¢å¹¶å‘ä¿®æ”¹
            locked_analysis = AIAnalysis.objects.select_for_update().get(id=analysis.id)
            
            locked_analysis.title = title
            locked_analysis.description = description
            locked_analysis.prompt = prompt

            # é‡è¦ï¼šç¡®ä¿model_usedå­—æ®µä¸ä¼šè¢«è¦†ç›–
            if original_model_name:
                locked_analysis.model_used = original_model_name
                logger.info(f"ğŸ’¾ [PROCESS] å¼ºåˆ¶è®¾ç½®æ¨¡å‹åç§°ä¸º: {original_model_name}")

            logger.info(f"ğŸ’¾ [PROCESS] ä¿å­˜åˆæ­¥åˆ†æè®°å½•...")
            logger.info(f"ğŸ’¾ [PROCESS] åˆæ­¥åˆ†æç»“æœ:")
            logger.info(f"  - æ ‡é¢˜: {title}")
            logger.info(f"  - æè¿°é•¿åº¦: {len(description)} å­—ç¬¦")
            logger.info(f"  - åˆå§‹æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æ ‡ç­¾æ•°é‡: {len(tag_names)} ä¸ª")
            logger.info(f"  - åˆ†ç±»æ•°é‡: {len(category_names)} ä¸ª")

            locked_analysis.save()
            logger.info(f"ğŸ’¾ [PROCESS] åˆæ­¥åˆ†æè®°å½•ä¿å­˜æˆåŠŸ: id={locked_analysis.id}")

            # ç¬¬äºŒé˜¶æ®µï¼šä¸“é—¨ä¼˜åŒ–æç¤ºè¯
            logger.info(f"ğŸ¨ [PROCESS] å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šä¼˜åŒ–æç¤ºè¯...")
            try:
                # è·å–æ¨¡å‹ç«¯ç‚¹URL
                from .models import AIModel
                ai_model = AIModel.objects.filter(name=original_model_name).first()
                if ai_model:
                    endpoint_url = ai_model.endpoint.url
                    # é‡æ–°ç¼–ç å›¾ç‰‡
                    image_path = analysis.media.file.path
                    image_data = _encode_image(image_path)

                    # æ„å»ºä¸“é—¨ç”¨äºæç¤ºè¯çš„æç¤ºè¯
                    prompt_only_prompt = _build_prompt_only_prompt(analysis.media, {
                        'title': title,
                        'description': description
                    })

                    # è°ƒç”¨APIç”Ÿæˆæ–°çš„æç¤ºè¯
                    optimized_prompt = _call_ollama_for_prompt_only(
                        endpoint_url,
                        original_model_name,
                        image_data,
                        prompt_only_prompt
                    )

                    if optimized_prompt:
                        logger.info(f"ğŸ¨ [PROCESS] æç¤ºè¯ä¼˜åŒ–æˆåŠŸï¼ŒåŸé•¿åº¦={len(prompt)}, æ–°é•¿åº¦={len(optimized_prompt)}")
                        locked_analysis.prompt = optimized_prompt
                        prompt = optimized_prompt  # æ›´æ–°æœ¬åœ°å˜é‡
                    else:
                        logger.warning(f"ğŸ¨ [PROCESS] æç¤ºè¯ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æç¤ºè¯")
                else:
                    logger.warning(f"ğŸ¨ [PROCESS] æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œè·³è¿‡æç¤ºè¯ä¼˜åŒ–")
            except Exception as e:
                logger.error(f"ğŸ¨ [PROCESS] æç¤ºè¯ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {str(e)}", exc_info=True)

            locked_analysis.status = 'completed'
            locked_analysis.analyzed_at = timezone.now()

            logger.info(f"ğŸ’¾ [PROCESS] æœ€ç»ˆä¿å­˜åˆ†æè®°å½•...")
            locked_analysis.save()
            logger.info(f"ğŸ’¾ [PROCESS] åˆ†æè®°å½•ä¿å­˜æˆåŠŸ: id={locked_analysis.id}, status={locked_analysis.status}, model_used={locked_analysis.model_used}")
            
            # æ›´æ–°analysiså¼•ç”¨ä»¥ä½¿ç”¨é”å®šåçš„å¯¹è±¡
            analysis = locked_analysis

            # è·å–ç”¨æˆ·IDç”¨äºåˆ›å»ºæ ‡ç­¾å’Œåˆ†ç±»
            user_id = analysis.media.user_id

            # å¤„ç†æ ‡ç­¾
            if tag_names:
                logger.info(f"ğŸ’¾ [PROCESS] å¤„ç†æ ‡ç­¾: {tag_names}")
                tags = []
                for tag_name in tag_names:
                    tag, created = Tag.objects.get_or_create(
                        name=tag_name.strip(),
                        user_id=user_id
                    )
                    tags.append(tag)
                    logger.info(f"ğŸ’¾ [PROCESS] æ ‡ç­¾ '{tag_name}': {'åˆ›å»ºæ–°æ ‡ç­¾' if created else 'ä½¿ç”¨ç°æœ‰æ ‡ç­¾'}")
                analysis.suggested_tags.add(*tags)
                logger.info(f"ğŸ’¾ [PROCESS] æ ‡ç­¾æ·»åŠ å®Œæˆï¼Œæ•°é‡: {len(tags)}")
            else:
                logger.info(f"ğŸ’¾ [PROCESS] æ²¡æœ‰æ ‡ç­¾éœ€è¦å¤„ç†")

            # å¤„ç†åˆ†ç±»
            if category_names:
                logger.info(f"ğŸ’¾ [PROCESS] å¤„ç†åˆ†ç±»: {category_names}")
                categories = []
                for category_name in category_names:
                    category, created = Category.objects.get_or_create(
                        name=category_name.strip(),
                        user_id=user_id
                    )
                    categories.append(category)
                    logger.info(f"ğŸ’¾ [PROCESS] åˆ†ç±» '{category_name}': {'åˆ›å»ºæ–°åˆ†ç±»' if created else 'ä½¿ç”¨ç°æœ‰åˆ†ç±»'}")
                analysis.suggested_categories.add(*categories)
                logger.info(f"ğŸ’¾ [PROCESS] åˆ†ç±»æ·»åŠ å®Œæˆï¼Œæ•°é‡: {len(categories)}")
            else:
                logger.info(f"ğŸ’¾ [PROCESS] æ²¡æœ‰åˆ†ç±»éœ€è¦å¤„ç†")

            # è®°å½•åº”ç”¨å‰çš„åª’ä½“æ–‡ä»¶çŠ¶æ€
            logger.info(f"ğŸ’¾ [PROCESS] åº”ç”¨å‰åª’ä½“æ–‡ä»¶: title='{analysis.media.title}', has_description={bool(analysis.media.description)}, has_prompt={bool(analysis.media.prompt)}")
            logger.info(f"ğŸ’¾ [PROCESS] åº”ç”¨å‰åª’ä½“æ–‡ä»¶åˆ†ç±»æ•°é‡: {analysis.media.categories.count()}, æ ‡ç­¾æ•°é‡: {analysis.media.tags.count()}")

            # è‡ªåŠ¨åº”ç”¨åˆ°åª’ä½“æ–‡ä»¶
            logger.info(f"ğŸ’¾ [PROCESS] å¼€å§‹åº”ç”¨åˆ†æç»“æœåˆ°åª’ä½“æ–‡ä»¶...")
            apply_result = analysis.apply_to_media()
            logger.info(f"ğŸ’¾ [PROCESS] åº”ç”¨ç»“æœ: {apply_result}")

            # è®°å½•åº”ç”¨åçš„åª’ä½“æ–‡ä»¶çŠ¶æ€
            logger.info(f"ğŸ’¾ [PROCESS] åº”ç”¨ååª’ä½“æ–‡ä»¶: title='{analysis.media.title}', has_description={bool(analysis.media.description)}, has_prompt={bool(analysis.media.prompt)}")
            logger.info(f"ğŸ’¾ [PROCESS] åº”ç”¨ååª’ä½“æ–‡ä»¶åˆ†ç±»æ•°é‡: {analysis.media.categories.count()}, æ ‡ç­¾æ•°é‡: {analysis.media.tags.count()}")

            logger.info(f"ğŸ’¾ [PROCESS] åˆ†æç»“æœå¤„ç†å®Œæˆ: {analysis.media.file.name}")

    except Exception as e:
        logger.error(f"ğŸ’¾ [PROCESS] ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise


def create_analysis_task(media_id: int, user_id: int, model_name: Optional[str] = None, options: Optional[Dict[str, Any]] = None) -> AIAnalysis:
    """
    åˆ›å»ºå›¾ç‰‡åˆ†æä»»åŠ¡

    Args:
        media_id: åª’ä½“æ–‡ä»¶ID
        user_id: ç”¨æˆ·ID
        model_name: æŒ‡å®šçš„æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        options: åˆ†æé€‰é¡¹ï¼ˆå¯é€‰ï¼‰

    Returns:
        AIAnalysis: åˆ†æè®°å½•å¯¹è±¡
    """
    logger.info(f"ğŸ¯ [CREATE] å¼€å§‹åˆ›å»ºå›¾ç‰‡åˆ†æä»»åŠ¡: media_id={media_id}, user_id={user_id}, model_name={model_name}, options={options}")

    try:
        with transaction.atomic():
            # è·å–åª’ä½“æ–‡ä»¶
            media = Media.objects.get(id=media_id, user_id=user_id)
            logger.info(f"ğŸ“ [CREATE] è·å–åª’ä½“æ–‡ä»¶æˆåŠŸ: ID={media.id}, æ–‡ä»¶å={media.file.name}, ç±»å‹={media.file_type}")

            if media.file_type != 'image':
                logger.error(f"âŒ [CREATE] ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {media.file_type}")
                raise ValueError("åªæ”¯æŒåˆ†æå›¾ç‰‡æ–‡ä»¶")

            # ä½¿ç”¨select_for_updateç¡®ä¿åŸå­æ€§æ“ä½œ
            analysis, created = AIAnalysis.objects.select_for_update().get_or_create(
                media=media,
                defaults={
                    'status': 'pending',
                    'model_used': model_name,
                    'analysis_options': options,
                }
            )

            if created:
                logger.info(f"âœ¨ [CREATE] åˆ›å»ºæ–°åˆ†æè®°å½•: ID={analysis.id}, model_used={analysis.model_used}")
            else:
                logger.info(f"ğŸ”„ [CREATE] é‡ç”¨ç°æœ‰åˆ†æè®°å½•: ID={analysis.id}, åŸæ¨¡å‹={analysis.model_used}")
                # å¦‚æœå·²å­˜åœ¨ï¼Œé‡ç½®çŠ¶æ€å¹¶æ›´æ–°é€‰é¡¹
                analysis.status = 'pending'
                analysis.error_message = None
                analysis.task_id = None
                analysis.model_used = model_name  # æ›´æ–°æ¨¡å‹åç§°
                analysis.analysis_options = options
                logger.info(f"ğŸ”„ [CREATE] å‡†å¤‡æ›´æ–°æ¨¡å‹: {analysis.model_used} -> {model_name}")
                analysis.save(update_fields=['status', 'error_message', 'task_id', 'model_used', 'analysis_options'])
                logger.info(f"ğŸ”„ [CREATE] ä¿å­˜åé‡æ–°æ£€æŸ¥: æ¨¡å‹={analysis.model_used}")
                logger.info(f"ğŸ”„ [CREATE] å·²é‡ç½®åˆ†æçŠ¶æ€å¹¶æ›´æ–°é€‰é¡¹ï¼Œæ¨¡å‹={model_name}")

            # ä½¿ç”¨ django-async-manager å¯åŠ¨ä»»åŠ¡
            logger.info(f"âš¡ [CREATE] å‡†å¤‡å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å‡½æ•°...")
            # ç›´æ¥è°ƒç”¨åå°ä»»åŠ¡å‡½æ•°
            task_instance = analyze_image(analysis.id)

            # ä¿å­˜ä»»åŠ¡IDï¼Œä½†ä¸æ”¹å˜å…¶ä»–å­—æ®µ
            analysis.task_id = str(task_instance.id)
            analysis.save(update_fields=['task_id'])
            logger.info(f"âœ… [CREATE] ä»»åŠ¡IDä¿å­˜æˆåŠŸ: task_id={analysis.task_id}")
            logger.info(f"ğŸ‰ [CREATE] å›¾ç‰‡åˆ†æä»»åŠ¡åˆ›å»ºå®Œæˆ: media_id={media_id}, task_id={analysis.task_id}, model_used={analysis.model_used}")

            return analysis

    except Exception as e:
        logger.error(f"âŒ [CREATE] åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        raise


def get_task_status(task_id: str) -> Dict[str, Any]:
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        from django_async_manager import get_task
        Task = get_task()
        logger.info(f"ğŸ” [STATUS] æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€: task_id={task_id}")

        try:
            task = Task.objects.get(id=task_id)
            logger.info(f"ğŸ” [STATUS] æ‰¾åˆ°ä»»åŠ¡: status={task.status}, attempts={task.attempts}/{task.max_retries}")

            # æ ¹æ®django-async-managerçš„å®é™…çŠ¶æ€æ˜ å°„
            if task.status == 'pending':
                result = {
                    'status': 'pending',
                    'progress': 0,
                    'is_task_running': False
                }
            elif task.status == 'in_progress':
                result = {
                    'status': 'processing',
                    'progress': 50,
                    'is_task_running': True
                }
            elif task.status == 'completed':
                result = {
                    'status': 'completed',
                    'progress': 100,
                    'is_task_running': False
                }
            elif task.status == 'failed':
                error_msg = 'Task failed'
                if task.last_errors and len(task.last_errors) > 0:
                    error_msg = task.last_errors[-1]
                result = {
                    'status': 'failed',
                    'progress': 0,
                    'is_task_running': False,
                    'error': error_msg
                }
            elif task.status == 'canceled':
                result = {
                    'status': 'failed',
                    'progress': 0,
                    'is_task_running': False,
                    'error': 'Task was canceled'
                }
            else:
                # å¤„ç†æœªçŸ¥çŠ¶æ€
                result = {
                    'status': 'processing' if task.attempts > 0 else 'pending',
                    'progress': 25 if task.attempts > 0 else 0,
                    'is_task_running': task.status not in ['completed', 'failed', 'canceled']
                }

            logger.info(f"ğŸ” [STATUS] è¿”å›çŠ¶æ€æ˜ å°„: {result}")
            return result

        except Task.DoesNotExist:
            logger.warning(f"ğŸ” [STATUS] ä»»åŠ¡ä¸å­˜åœ¨: task_id={task_id}")
            return {
                'status': 'not_found',
                'progress': 0,
                'is_task_running': False,
                'error': 'Task not found'
            }

    except Exception as e:
        logger.error(f"ğŸ” [STATUS] è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        return {
            'status': 'error',
            'progress': 0,
            'is_task_running': False,
            'error': str(e)
        }
