"""
æ”¹è¿›çš„æ‰¹é‡å¤„ç†å™¨ - ç®€åŒ–ç‰ˆ
ç§»é™¤å¤æ‚çš„å¹¶å‘æ§åˆ¶ï¼Œåªè´Ÿè´£ä»»åŠ¡å‡†å¤‡å’Œç»“æœæ•´ç†
"""

import logging
from typing import Dict, Any, List
from django.db import transaction
from .state_manager import state_manager

logger = logging.getLogger(__name__)


class SimplifiedBatchHandler:
    """ç®€åŒ–ç‰ˆæ‰¹é‡å¤„ç†å™¨"""

    def __init__(self):
        self.max_batch_size = 50  # å¢åŠ æ‰¹é‡å¤§å°

    def prepare_tasks(self, user, media_ids, model_name=None, analysis_options=None, prompt=None):
        """å‡†å¤‡æ‰¹é‡ä»»åŠ¡ - ç®€åŒ–ç‰ˆ"""
        from media.models import Media
        from ollama.models import OllamaAIModel

        valid_tasks = []
        validation_errors = []

        # æ‰¹é‡æŸ¥è¯¢åª’ä½“æ–‡ä»¶
        media_items = Media.objects.filter(
            id__in=media_ids,
            user=user
        ).select_related('user')

        # æ£€æŸ¥ç¼ºå¤±çš„åª’ä½“
        found_ids = {m.id for m in media_items}
        for media_id in media_ids:
            if media_id not in found_ids:
                validation_errors.append({
                    'media_id': media_id,
                    'error': 'åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æƒè®¿é—®'
                })

        # è·å–æ¨¡å‹
        model = self._get_model(user, model_name)
        if not model:
            raise BatchValidationError("æ²¡æœ‰å¯ç”¨çš„åˆ†ææ¨¡å‹")

        # æ‰¹é‡åˆ›å»ºåˆ†æä»»åŠ¡
        for media in media_items:
            try:
                analysis, created = state_manager.create_analysis_safely(
                    media=media,
                    model=model,
                    analysis_options=analysis_options or {},
                    prompt=prompt
                )

                valid_tasks.append(analysis)
                logger.debug(f"åˆ›å»ºåˆ†æä»»åŠ¡: media_id={media.id}, analysis_id={analysis.id}")

            except Exception as e:
                validation_errors.append({
                    'media_id': media.id,
                    'error': f"åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}"
                })

        summary = {
            'total_requested': len(media_ids),
            'valid_tasks': len(valid_tasks),
            'validation_errors': len(validation_errors)
        }

        return valid_tasks, validation_errors, summary

    def _get_model(self, user, model_name=None):
        """è·å–æ¨¡å‹ - ç®€åŒ–ç‰ˆ"""
        from ollama.models import OllamaAIModel

        queryset = OllamaAIModel.objects.filter(
            endpoint__created_by=user,
            is_active=True,
            is_vision_capable=True
        )

        if model_name:
            queryset = queryset.filter(name=model_name)

        return queryset.filter(is_default=True).first() or queryset.first()

    def analyze_images_with_concurrency_task(self, user_id, media_ids, model_name, analysis_options=None, prompt=None):
        """
        å›¾ç‰‡å¹¶å‘æ‰¹é‡åˆ†æä»»åŠ¡ - ç®€åŒ–ç‰ˆ
        ç›´æ¥ä½¿ç”¨å¹¶å‘ç®¡ç†å™¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ: {len(media_ids)} å¼ å›¾ç‰‡ï¼Œç”¨æˆ·: {user_id}")

        try:
            from django.contrib.auth import get_user_model
            User = get_user_model()

            # è·å–ç”¨æˆ·
            user = User.objects.get(id=user_id)

            # éªŒè¯è¯·æ±‚
            validation_result = self.validate_request(media_ids, model_name, analysis_options)
            if not validation_result['valid']:
                return {
                    'success': False,
                    'error': f"æ‰¹é‡è¯·æ±‚éªŒè¯å¤±è´¥: {'; '.join(validation_result['errors'])}"
                }

            # å‡†å¤‡ä»»åŠ¡
            valid_tasks, validation_errors, summary = self.prepare_tasks(
                user=user,
                media_ids=media_ids,
                model_name=model_name,
                analysis_options=analysis_options,
                prompt=prompt
            )

            if not valid_tasks:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰å¯å¤„ç†çš„ä»»åŠ¡',
                    'validation_errors': validation_errors
                }

            # åˆ›å»ºæ‰¹é‡åˆ†æå¼‚æ­¥ä»»åŠ¡
            from .task_workers import analyze_batch_task

            analysis_ids = [task.id for task in valid_tasks]
            max_concurrent = analysis_options.get('max_concurrent', 10) if analysis_options else 10

            batch_task = analyze_batch_task.run_async(
                user_id=user_id,
                analysis_ids=analysis_ids,
                model_name=model_name,
                max_concurrent=max_concurrent
            )

            logger.info(f"âœ… æ‰¹é‡åˆ†æä»»åŠ¡å·²å¯åŠ¨: task_id={batch_task.id}")

            return {
                'success': True,
                'batch_started': True,
                'batch_task_id': str(batch_task.id),
                'summary': summary,
                'analysis_ids': analysis_ids,
                'max_concurrent': max_concurrent
            }

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ†æä»»åŠ¡å¼‚å¸¸: {str(e)}")
            return {
                'success': False,
                'error': f"æ‰¹é‡åˆ†æä»»åŠ¡å¼‚å¸¸: {str(e)}"
            }

    def validate_request(self, media_ids, model_name=None, analysis_options=None):
        """éªŒè¯æ‰¹é‡è¯·æ±‚å‚æ•°"""
        errors = []
        warnings = []

        # éªŒè¯åª’ä½“IDåˆ—è¡¨
        if not media_ids or not isinstance(media_ids, list):
            errors.append("media_ids å¿…é¡»æ˜¯éç©ºæ•°ç»„")
            return {'valid': False, 'errors': errors, 'warnings': warnings}

        if len(media_ids) > self.max_batch_size:
            errors.append(f"æ‰¹é‡å¤§å°è¶…è¿‡é™åˆ¶ï¼Œæœ€å¤šæ”¯æŒ {self.max_batch_size} ä¸ªæ–‡ä»¶")

        if len(media_ids) == 0:
            errors.append("åª’ä½“IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        # æ£€æŸ¥é‡å¤ID
        if len(media_ids) != len(set(media_ids)):
            warnings.append("åª’ä½“IDåˆ—è¡¨ä¸­åŒ…å«é‡å¤é¡¹")

        # éªŒè¯å¹¶å‘æ§åˆ¶å‚æ•°
        analysis_options = analysis_options or {}
        if 'max_concurrent' in analysis_options:
            max_concurrent = analysis_options['max_concurrent']
            if not isinstance(max_concurrent, int) or not 1 <= max_concurrent <= 10:
                errors.append('max_concurrentå¿…é¡»åœ¨1-10ä¹‹é—´')

        # éªŒè¯æ¨¡å‹åç§°
        if model_name and not isinstance(model_name, str):
            errors.append("æ¨¡å‹åç§°å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings,
            'media_count': len(media_ids)
        }

    def execute_processing(self, user, valid_tasks, analysis_options):
        """ç®€åŒ–çš„æ‰¹é‡æ‰§è¡Œå¤„ç†"""
        from .concurrency_manager import concurrency_manager

        media_ids = [task.media.id for task in valid_tasks]

        return concurrency_manager.process_batch_images(
            user_id=user.id,
            media_ids=media_ids,
            model_name=valid_tasks[0].model.name,
            analysis_options=analysis_options
        )


class BatchError(Exception):
    """æ‰¹é‡å¤„ç†é”™è¯¯åŸºç±»"""
    pass


class BatchValidationError(BatchError):
    """æ‰¹é‡å¤„ç†éªŒè¯é”™è¯¯"""
    pass


# æ›´æ–°å…¨å±€å®ä¾‹
batch_handler = SimplifiedBatchHandler()